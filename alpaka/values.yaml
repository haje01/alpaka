#
# 밸류 파일 내 값은 기본 값으로 동작 
# 

# 쿠버네티스 배포판 이름 (minibkue, k3s, k3d, eks 중 하나)
k8s_dist: UNKNOWN

#
# 주의: RELEASE 가들어간 이름은 설치용 설정 파일에서 배포 이름으로 대체되어야함.
#

# 카프카 설정
kafka:
  replicaCount: 1
  defaultReplicationFactor: 1
  numPartitions: 8
  ## 노드 리소스 조건
  # resources:
  #   requests:
  #     cpu: "2700m"
  #     memory: "10Gi"
  #   limits:
  #     cpu: "3600m"
  #     memory: "14Gi"

  # jvm_ 메트릭을 얻기 위해서는 java agent 로 실행해야하나 잘 되지 않음.. ㅠ
  # https://stackoverflow.com/questions/51372578/no-kafka-metrics-in-grafana-prometheus
  # https://github.com/bitnami/charts/pull/7012+      
  
  metrics:
    kafka:
      enabled: true
    jmx:
      enabled: true
    serviceMonitor:
      enabled: true
      interval: 10s
      scrapeTimeout: 5s
      relabelings:
        - sourceLabels: ["container", "pod"]
          regex: "jmx-exporter;"
          replacement: jmx
          targetLabel: job
        # 인스턴스 값을 IP 가 아닌 Pod 명으로 
        - regex: "instance"
          action: labeldrop
        - sourceLabels: ["pod"]
          targetLabel: instance
      metricRelabelings:
        # 인스턴스 값을 IP 가 아닌 Pod 명으로 
        - regex: "instance"
          action: labeldrop
        - sourceLabels: ["pod"]
          targetLabel: instance

  zookeeper:
    enabled: true
    replicaCount: 1
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
        interval: 10s
        scrapeTimeout:  5s
        relabelings: 
          - replacement: zookeeper
            targetLabel: job
          # 인스턴스 값을 IP 가 아닌 Pod 명으로 
          - regex: "instance"
            action: labeldrop
          - sourceLabels: ["pod"]
            targetLabel: instance
        metricRelabelings:
          # 인스턴스 값을 IP 가 아닌 Pod 명으로 
          - regex: "instance"
            action: labeldrop
          - sourceLabels: ["pod"]
            targetLabel: instance

  deleteTopicEnable: true
  podLabels:
    job: kafka


# 프로메테우스 설정
prometheus:
  enabled: true
  prometheus:
    enabled: true
    additionalScrapeConfigs:
      enabled: true
      type: internal
      internal:
        jobList:
        - job_name: kminion-metrics
          scrape_interval: 10s
          scrape_timeout:  5s
          metrics_path: "/metrics"
          static_configs:
          - targets:
            - RELEASE-kminion:8080
        - job_name: zookeeper
          scrape_interval: 10s
          scrape_timeout:  5s
          metrics_path: "/metrics"
          static_configs:
          - targets:
            - RELEASE-zookeeper-metrics:9141


# 쿠버네티스 대쉬보드 설정
k8dashboard:
  enabled: true
  protocolHttp: true 
  service:
    externalPort: 8443
  serviceAccount:
    name: k8dash-admin
  extraArgs:
    - --token-ttl=86400
    - --enable-skip-login 
    - --enable-insecure-login
  tolerations:
  - key: type
    operator: "Equal"
    value: "ctrl"
    effect: "NoSchedule"


# 알파카 Tool 설정
tool:
  enabled: true
  name: alpaka-tool
  podLabels: {}
  podAnnotations: {}
  container:
    image: "haje01/alpaka-tool"
    tag: 0.0.6
    pullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  dnsConfig:
    # 필요시 외부 DNS 등록 
    # nameservers:
    #   - 213.234.76.4
    options:
      - name: ndots
        value: "2"


# 카프카 커넥트 설정
kafka_connect:
  enabled: false
  connects: []
  # # 소스 커넥트 예시
  # - type: srccon 
  #   replicaCount: 
  #   container:
  #     image: "haje01/kafka-srccon"
  #     tag: 0.0.6
  #     pullPolicy: IfNotPresent
  #   nodeSelector: 
  #     alpaka/node-type: worker
  #   timezone: Asia/Seoul 
  #   # 커넥트 공유 변수
  #   values:
  #     db_ip: my-mssql-db-addr
  #     db_name: person
  #     db_user: myuser
  #     db_pass: mypass
  #   connector_groups:
  #   # 모카 로그 그룹 
  #   - name: mocca_log
  #     # 커넥터 공통 설정
  #     common:
  #       connector.class: io.confluent.connect.jdbc.JdbcSourceConnector
  #       tasks.max: "1"
  #       connection.url: "jdbc:sqlserver://{{ .Values.db_ip }};databaseName={{ .Values.db_name }}"
  #       connection.password: "{{ .Values.db_pass }}"
  #       connection.attempts: 288       # retry정책: 288번(24시간) # default: 3
  #       connection.backoff.ms: 300000  # retry정책: 5분마다       # default: 10초
  #       db.timezone: "Asia/Seoul"
  #       value.converter: "org.apache.kafka.connect.json.JsonConverter"
  #       value.converter.schemas.enable: false
  #       poll.interval.ms: 20000        # default: 60000ms (1분)
  #       batch.max.rows: 2000           # default: 100개
  #       transaction.isolation.ms: READ_UNCOMMITED  # sql server default: READ_COMMITED
  #     # 커넥터별 설정 
  #     connectors:
  #     - name: "T_PurchaseLog"
  #       config:
  #         topic.prefix: kst_T_PurchaseLog4
  #         mode: incrementing
  #         incrementing.column.name: LogNo4
  #         poll.interval.ms: 30000
  #         query:  # connect_offset 처리 대신 where로 수집대상범위 설정
  #           "select * from
  #           (
  #           select * from T_PurchaseLog4
  #           where RegDate >= CONVERT(DATETIME, '2022-11-29 00:00:00.000')
  #           )AS T"
  #         numeric.mapping: "best_fit" # Numeric 컬럼 처리

  #     - name: "T_PurchaseProcessLog"
  #       config:
  #         topic.prefix: kst_T_PurchaseProcessLog
  #         mode: incrementing
  #         incrementing.column.name: LogNo
  #         query:  # connect_offset 처리 대신 where로 수집대상범위 설정
  #           "select * from
  #           (
  #           select * from T_PurchaseProcessLog
  #           where RegDate >= CONVERT(DATETIME, '2022-11-29 00:00:00.000')
  #           )AS T"
  #         numeric.mapping: "best_fit" # Numeric 컬럼 처리



# UI for 카프카 설정
ui4kafka:
  enabled: true
  yamlApplicationConfig:
    kafka:
      clusters:
      - name: RELEASE-kafka
        bootstrapServers: RELEASE-kafka-headless:9092
        zookeeper: RELEASE-zookeeper-headless:2181
        kafkaConnect:
        #
        # 주의 : 이 아래 섹션을 kafka_connect.connects 에 등록한 connect 에 대해 순회
        # CONNECT_TYPE 은 kafka_connect.connects.type 
        #
        - name: CONNECT_TYPE
          address: http://RELEASE-alpaka-CONNECT_TYPE:8083
        metrics:
          port: 5556
          type: JMX
    auth:
      type: disabled
    management:
      health:
        ldap:
          enabled: false
  tolerations:
  - key: type
    operator: "Equal"
    value: "ctrl"
    effect: "NoSchedule"


ksqldb:
  enabled: false
  ksqldb:
    nameOverride: RELEASE-ksqldb
    kafka:
      enabled: false 
      bootstrapServer: PLAINTEXT://RELEASE-kafka-headless:9092
    schema-registry: 
      enabled: false 
    kafka-connect:
      enabled: false 
  resources:
    limits:
      # 차트 기본 설정인 2000Mi 가 부족해 OOMKilled 발생
      memory: 4000Mi


# 그라파나 설정
grafana:
  enabled: true
  adminSecretName: RELEASE-grafana-admin
  admin:
    user: admin
    password: admindjemals
  datasources:
    secretDefinition:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          access: proxy
          url: http://RELEASE-prometheus-prometheus:9090
          isDefault: true
  dashboardsProvider:
    enabled: true
  dashboardsConfigMaps:
    - configMapName: RELEASE-alpaka-grafana-cluster
      # KMinion 카프카 클러스터 대쉬보드 
      fileName: kminion-cluster_rev1.json             
    - configMapName: RELEASE-alpaka-grafana-topic
      # KMinion 카프카 토픽 대쉬보드 
      fileName: kminion-topic_rev1.json
    - configMapName: RELEASE-alpaka-grafana-groups
      # KMinion 카프카 컨슈머 그룹 대쉬보드 
      fileName: kminion-groups_rev1.json
    - configMapName: RELEASE-alpaka-grafana-zookeeper
      # 주키퍼 대쉬보드 
      fileName: zookeeper-by-prometheus_rev4.json
    - configMapName: RELEASE-alpaka-grafana-jvm
      # 카프카 브로커의 JMX 대쉬보드 
      fileName: altassian-overview_rev1.json
    - configMapName: RELEASE-alpaka-grafana-kafka-exporter
      # 카프카 브로커의 JMX 대쉬보드 
      fileName: kafka-exporter-overview_rev5.json


# 그라파나용 KMinion 대쉬보드 설정
kminion:
  enabled: true
  kminion:
    config:
      kafka:
        brokers: ["RELEASE-kafka-headless:9092"]

    exporter:
      host: "RELEASE-kminion"
      port: 8080

    # annotations:
    #   prometheus.io/probe: kminion


# 공용 ingress 설정
ingress:
  enabled: true
  annotations:
    # # minikube 설치용
    # kubernetes.io/ingress.class: nginx

    # # k3s 설치용
    # kubernetes.io/ingress.class: traefik

    # # k3d 설치용
    # ingress.kubernetes.io/ssl-redirect: "false"

    # # AWS EKS 설치용
    # kubernetes.io/ingress.class: alb
    # alb.ingress.kubernetes.io/group.name: public
    # alb.ingress.kubernetes.io/scheme: internet-facing
    # alb.ingress.kubernetes.io/target-type: ip
    # alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 9090}]'


#
# 초기화 
#
# 설치 후 초기화에 필요한 명령을 정의하고 실행한다.
# 
init: 
  enabled: false 
  container:
    image: "haje01/alpaka-tool"
    tag: 0.0.6
    pullPolicy: IfNotPresent
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  files: {}
  commands: []


#
#  테스트 관련
#

#
# 테스트용 MySQL 정보
#
# MySQL 은 test.enabled 가 true 인 경우만 설치된다.
#
mysql:
  nameOverride: test-mysql
  auth:
    password: mypass
    rootPassword: mypass

# 테스트
test:
  enabled: true
  # 테스트 컨테이너
  container:
    image: "haje01/alpaka-tool"
    tag: 0.0.6
    pullPolicy: IfNotPresent
  # 테스트용 카프카 커넥트별 정보
  connects:
    # 소스 커넥터
    srccon:
      # nodeSelector:
      #   alpaka/node-type: worker
      container:
        image: "haje01/kafka-srccon"
        tag: 0.0.6
        pullPolicy: IfNotPresent
    # 싱크 커넥터 
    sinkcon:
      # nodeSelector:
      #   alpaka/node-type: worker
      container:
        image: "haje01/kafka-sinkcon"
        tag: 0.0.3
        pullPolicy: IfNotPresent
  # S3 싱크 커넥터 테스트 정보 
  s3sink:
    enabled: false
    # bucket: S3 싱크 커넥터가 사용할 버킷
    # topics_dir: S3 싱크 커넥터가 사용할 버킷내 디렉토리 
    # region: S3 싱크 커넥터가 사용할 AWS 리전
  envs: []
  # - name: AWS_ACCESS_KEY_ID
  #   value: 액세스 키값
  # - name: AWS_SECRET_ACCESS_KEY 
  #   value: 시크릿 키값
  # - name: AWS_DEFAULT_REGION
  #   value: ap-northeast-2
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
